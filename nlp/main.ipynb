{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "corpus = set(stopwords.words('english'))\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_code_blocks(string):\n",
    "    list_block = string.split('```')\n",
    "    new_list_block = []\n",
    "    for index, item in enumerate(list_block):\n",
    "        if index % 2 == 0:\n",
    "            new_list_block.append(item)\n",
    "    \n",
    "    return ' '.join(new_list_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_basic_clean(string):\n",
    "    \"\"\"\n",
    "    Lowercases, removes non-ASCII characters, and removes non-alphanumeric (except ' or \\s') from the passed in string.\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_string = string\n",
    "    \n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    cleaned_string = unicodedata.normalize(\"NFKD\", cleaned_string).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "    cleaned_string = re.sub('<[^<]+?>', ' ', cleaned_string)\n",
    "    cleaned_string = re.sub(r\"\\n\", \" \", cleaned_string)\n",
    "    cleaned_string = re.sub(r'http\\S+', ' ', cleaned_string)\n",
    "    cleaned_string = remove_code_blocks(cleaned_string)\n",
    "    cleaned_string = re.sub(r\"[^a-z'\\s]\", ' ', cleaned_string)\n",
    "    cleaned_string = re.sub(r\"\\s\\s+\" , ' ', cleaned_string)\n",
    "    cleaned_string = cleaned_string.strip()\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    file = open(file_path, mode='r', encoding=\"utf8\")\n",
    "    all_of_it = file.read()\n",
    "    file.close()\n",
    "    return all_of_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(string):\n",
    "    token_list = word_tokenize(string)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def text_lemmatizing(val):\n",
    "    datas = val\n",
    "    for index, data in enumerate(datas):\n",
    "        datas[index] = lemmatizer.lemmatize(data)\n",
    "    return datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenize_list):\n",
    "    tokenize_list = [x for x in tokenize_list if x not in corpus]\n",
    "    return tokenize_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "def freq_text(token_list):\n",
    "    fdist = FreqDist(token_list)\n",
    "    return fdist\n",
    "\n",
    "def freq_perform(fdist):\n",
    "    print(max(fdist), max(fdist.values()))\n",
    "\n",
    "    fdist.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "\n",
    "def remove_singleton_doubleton(tokenize_list):\n",
    "    return [ i for i in tokenize_list if len(i) > 3 ]\n",
    "\n",
    "def remove_non_english_word(tokenize_list):\n",
    "    corpus_words = words.words()\n",
    "    return [ i for i in tokenize_list if i in corpus_words ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_clean_text(file_path):\n",
    "    text = read_text_file(file_path)\n",
    "    clean_text = nlp_basic_clean(text)\n",
    "    tokenize_list = tokenize_text(clean_text)\n",
    "    tokenize_list = text_lemmatizing(tokenize_list)\n",
    "    tokenize_list = remove_stopwords(tokenize_list)\n",
    "    tokenize_list = remove_singleton_doubleton(tokenize_list)\n",
    "    tokenize_list = remove_non_english_word(tokenize_list)\n",
    "    final_clean_text = ' '.join(tokenize_list)\n",
    "    return final_clean_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = get_final_clean_text('process/test1.txt')\n",
    "text2 = get_final_clean_text('process/test2.txt')\n",
    "text3 = get_final_clean_text('process/test3.txt')\n",
    "text4 = get_final_clean_text('process/test4.txt')\n",
    "text5 = get_final_clean_text('process/test5.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform([text1, text2, text3, text4, text5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_2 = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.0426953 , 0.15621225, 0.03170753, 0.1216691 ],\n",
       "       [0.0426953 , 1.        , 0.04657382, 0.        , 0.0386873 ],\n",
       "       [0.15621225, 0.04657382, 1.        , 0.11660882, 0.30921236],\n",
       "       [0.03170753, 0.        , 0.11660882, 1.        , 0.18091289],\n",
       "       [0.1216691 , 0.0386873 , 0.30921236, 0.18091289, 1.        ]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
